{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ec33402-d38b-4427-8994-b4411c6ff5ac",
   "metadata": {},
   "source": [
    "# Airbnb Revenue Prediction with HistGradientBoostingRegressor\n",
    "-------------------------------------------------------------\n",
    "\n",
    "This script complements the **XGBoost pipeline** by providing an alternative \n",
    "implementation using scikit-learnâ€™s **HistGradientBoostingRegressor (HBGR)**.\n",
    "\n",
    "It includes:\n",
    "- Preprocessing of numerical and categorical features\n",
    "- A HistGradientBoostingRegressor with tuned hyperparameters\n",
    "- Model evaluation (train/validation MAE)\n",
    "\n",
    "The purpose of this script is to **compare performance and interpretability** \n",
    "between HBGR and XGBoost. While XGBoost provides built-in feature importance scores, \n",
    "HBGR relies on **permutation importance** for model-agnostic interpretability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e881dd-cad0-458b-98e8-cd5fa02c50d5",
   "metadata": {},
   "source": [
    "## Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c035252b-443c-4d94-b4a8-6c3bd2e0d68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import json\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer, SimpleImputer\n",
    "\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83443081-31a5-41b7-8f5b-3e87564fbfa7",
   "metadata": {},
   "source": [
    "## Defining a baseline function containing HistGradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be81914f-b0fb-4ce0-8b74-26d58ded0070",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hist_baseline():\n",
    "\n",
    "    \"\"\"\n",
    "    Train a Histogram Gradient Boosting Regressor on the Airbnb dataset.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    best_model : Fitted pipeline with preprocessing & HistGradientBoostingRegressor.\n",
    "    grid_search : Grid search object if tuning was run, else None\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    logging.info(\"Reading train and test files\")\n",
    "    train = pd.read_json(\"train.json\", orient='records')\n",
    "    test = pd.read_json(\"test.json\", orient='records')\n",
    "    seed = 123\n",
    "\n",
    "    # Split train into train and validation\n",
    "    train, valid = train_test_split(train, test_size=1/3, random_state=seed)\n",
    "    \n",
    "    # Define preprocessing pipeline\n",
    "    preprocess = ColumnTransformer(\n",
    "        transformers=[\n",
    "            # Numerical features - impute then scale\n",
    "            (\"numerical\", Pipeline(steps=[\n",
    "                ('imputer', IterativeImputer(random_state=seed, max_iter=10)),\n",
    "                ('scaler', StandardScaler())\n",
    "            ]), [\"lat\", \"lon\", \"bathrooms\", \"rooms\", \"guests\", \"num_reviews\", \"rating\", \"min_nights\"]),\n",
    "            \n",
    "            # Categorical features - impute then one-hot encode\n",
    "            (\"categorical\", Pipeline(steps=[\n",
    "                ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "            ]), [\"room_type\", \"cancellation\"]),\n",
    "        ],\n",
    "        remainder='drop'\n",
    "    )\n",
    "\n",
    "    label = 'revenue'\n",
    "\n",
    "    hbg_regressor = Pipeline(steps=[\n",
    "        ('preprocess', preprocess), \n",
    "        ('HGB', HistGradientBoostingRegressor(\n",
    "            learning_rate=0.1,          \n",
    "            max_iter=300,               \n",
    "            max_depth=3,                 \n",
    "            min_samples_leaf=30,       \n",
    "            l2_regularization=1.0,      \n",
    "            max_features=0.7,           \n",
    "            validation_fraction=0.15,   \n",
    "            early_stopping=True,\n",
    "            n_iter_no_change=15,\n",
    "            random_state=seed\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "    X_train = train.drop([label], axis=1)\n",
    "    y_train = np.log1p(train[label].values)\n",
    "\n",
    "    # Optional: Uncomment this block to perform grid search tuning\n",
    "    # (Warning: VERY computationally expensive)\n",
    "    \n",
    "    '''\n",
    "    logging.info(\"Starting grid search for hyperparameter tuning...\")\n",
    "\n",
    "        # Create pipeline with basic model (parameters will be set by grid search)\n",
    "    hgb_regressor = Pipeline(steps=[\n",
    "        ('preprocess', preprocess), \n",
    "        ('HGB', HistGradientBoostingRegressor(\n",
    "            validation_fraction=0.15,   \n",
    "            early_stopping=True,\n",
    "            n_iter_no_change=10,\n",
    "            random_state=seed\n",
    "        ))\n",
    "    ])\n",
    "    \n",
    "    # Define parameter grid\n",
    "    param_grid = {\n",
    "        'HGB__learning_rate': [0.01, 0.05, 0.1],\n",
    "        'HGB__max_depth': [2, 3, 4],\n",
    "        'HGB__l2_regularization': [0.01, 0.1, 1.0],\n",
    "        'HGB__max_iter': [100, 200, 300],\n",
    "        'HGB__min_samples_leaf': [10, 20, 30]\n",
    "    }\n",
    "    \n",
    "    # Perform grid search\n",
    "    grid_search = GridSearchCV(\n",
    "        hgb_regressor, \n",
    "        param_grid, \n",
    "        cv=5,\n",
    "        scoring='neg_mean_absolute_error',\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Fit grid search\n",
    "    X_train = train.drop([label], axis=1)\n",
    "    y_train = np.log1p(train[label].values)\n",
    "    \n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Get best model\n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "\n",
    "    print(f'Best parameters: {grid_search.best_params_}')\n",
    "    print(f'Best CV score (neg MAE): {grid_search.best_score_:.4f}')\n",
    "    '''\n",
    "    \n",
    "    # Comment out if running grid search\n",
    "    hbg_regressor.fit(X_train, y_train)\n",
    "    best_model = hbg_regressor  \n",
    "    grid_search = None\n",
    "    \n",
    "    logging.info(\"Evaluating model performance...\")\n",
    "    \n",
    "    for split_name, split in [(\"Train\", train), (\"Valid\", valid)]:\n",
    "        X_split = split.drop([label], axis=1)\n",
    "        y_true = split[label].values\n",
    "        \n",
    "        # Predict and reverse log transform\n",
    "        y_pred_log = best_model.predict(X_split)\n",
    "        y_pred = np.expm1(y_pred_log)\n",
    "        \n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        \n",
    "        logging.info(f\"{split_name:>5} - MAE: {mae:.3f}\")\n",
    "\n",
    "    '''\n",
    "    # Make predictions on test set\n",
    "    logging.info(\"Generating test predictions...\")\n",
    "    pred_test_log = best_model.predict(test)\n",
    "    pred_test = np.expm1(pred_test_log)\n",
    "    \n",
    "    # Ensure no negative predictions\n",
    "    pred_test = np.maximum(pred_test, 0)\n",
    "    \n",
    "    test[label] = pred_test\n",
    "    predicted = test[['revenue']].to_dict(orient='records')\n",
    "\n",
    "    # Save predictions\n",
    "    logging.info(\"Saving predictions to baseline.zip...\")\n",
    "    with zipfile.ZipFile(\"baseline.zip\", \"w\", zipfile.ZIP_DEFLATED) as zipf:\n",
    "        zipf.writestr(\"predicted.json\", json.dumps(predicted, indent=2))\n",
    "    \n",
    "    logging.info(\"Pipeline completed successfully!\")\n",
    "    '''\n",
    "    \n",
    "    return best_model, grid_search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b31fe5d-cb2b-4980-8c95-c444eb1f70b8",
   "metadata": {},
   "source": [
    "## Defining a function to analyze permutation importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82ee4f15-31ff-4d67-9b17-63ba9e2ea50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def permutation_feature_importance(pipeline_model, X, y, seed=123, n_repeats=10):\n",
    "    \"\"\"\n",
    "    Compute permutation feature importance.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Transform features\n",
    "    preprocess = pipeline_model.named_steps['preprocess']\n",
    "    X_transformed = preprocess.transform(X)\n",
    "    \n",
    "    # Run permutation importance on the fitted model only\n",
    "    model = pipeline_model.named_steps['HGB']\n",
    "    perm_importance = permutation_importance(\n",
    "        model, X_transformed, y, n_repeats=n_repeats, random_state=seed, n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Get feature names after transformation\n",
    "    feature_names = preprocess.get_feature_names_out()\n",
    "    \n",
    "    # Build DataFrame\n",
    "    importances = pd.DataFrame({\n",
    "        \"feature\": feature_names,\n",
    "        \"importance_mean\": perm_importance.importances_mean,\n",
    "        \"importance_std\": perm_importance.importances_std\n",
    "    }).sort_values(\"importance_mean\", ascending=False)\n",
    "    \n",
    "    return importances\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c251fd-3456-45e5-9f94-8c414440d526",
   "metadata": {},
   "source": [
    "## Executing HistGradientBoostingRegressor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53a96f32-e369-4036-a02e-f1fdfd49e957",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-20 16:10:22,983 - INFO - Reading train and test files\n",
      "2025-08-20 16:10:23,483 - INFO - Evaluating model performance...\n",
      "2025-08-20 16:10:23,725 - INFO - Train - MAE: 8698.420\n",
      "2025-08-20 16:10:23,777 - INFO - Valid - MAE: 8801.490\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           feature  importance_mean  \\\n",
      "1                                   numerical__lon         0.326759   \n",
      "5                           numerical__num_reviews         0.285784   \n",
      "0                                   numerical__lat         0.122436   \n",
      "6                                numerical__rating         0.086098   \n",
      "3                                 numerical__rooms         0.051818   \n",
      "8               categorical__room_type_entire_home         0.043824   \n",
      "4                                numerical__guests         0.035536   \n",
      "2                             numerical__bathrooms         0.022467   \n",
      "7                            numerical__min_nights         0.010880   \n",
      "19  categorical__cancellation_Super Strict 60 Days         0.002624   \n",
      "\n",
      "    importance_std  \n",
      "1         0.008016  \n",
      "5         0.007269  \n",
      "0         0.006749  \n",
      "6         0.006236  \n",
      "3         0.002772  \n",
      "8         0.003783  \n",
      "4         0.002102  \n",
      "2         0.001827  \n",
      "7         0.001257  \n",
      "19        0.000346  \n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    best_model, _ = hist_baseline()\n",
    "\n",
    "    # Use your validation split for feature importance\n",
    "    valid = pd.read_json(\"train.json\", orient=\"records\")  # reload so we get the same split\n",
    "    train, valid = train_test_split(valid, test_size=1/3, random_state=123)\n",
    "\n",
    "    X_valid = valid.drop([\"revenue\"], axis=1)\n",
    "    y_valid = np.log1p(valid[\"revenue\"].values)  # keep consistent with training\n",
    "\n",
    "    fi = permutation_feature_importance(best_model, X_valid, y_valid)\n",
    "    print(fi.head(10))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
